BDM_HW1.pdf:
Description: This file includes exercises on setting up and using AWS EC2 to run a Linux terminal for various tasks, including Python programming and Hadoop simulation for word count operations.
AWS Usage: EC2 was used to run a Linux environment where the tasks were performed. 

BDM_HW2.pdf:
Description: This homework covers SQL queries and Hadoop MapReduce programs for different tasks such as calculating the largest integer, average, and counting distinct values from large datasets.
AWS Usage: EC2 instances would be employed to run Hadoop simulations, with S3 potentially used to store large datasets or intermediate results from the MapReduce tasks.

BDM_HW3_Final.pdf:
Description: This assignment involves setting up a multi-node Hadoop cluster to run word count and SQL queries. The performance of single-node vs multi-node cluster setups is compared.
AWS Usage: AWS EC2 instances were utilized to set up the Hadoop cluster.

BDM_HW4.pdf:
Description: This homework focuses on Bloom Filters and their applications, including matrix multiplication using Hadoop, and implementing PageRank algorithms for a given graph.
AWS Usage: EC2 instances were used to perform the distributed computations using Hadoop, with S3 used for storing large datasets and results from PageRank and matrix multiplication operations.

BDM_Final.pdf:
Description: The final exam project includes implementing K-Means clustering using Hadoop Streaming and conducting a Bloom Filter-based join operation between two large tables.
AWS Usage: EC2 was used to deploy Hadoop jobs for clustering and Bloom Filter-based joins. S3 being utilized for storing input datasets and output results. 
